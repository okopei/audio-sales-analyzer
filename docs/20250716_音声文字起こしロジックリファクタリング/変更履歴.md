# SpeechToTextPipeline リファクタリング変更履歴

## 2025年7月16日 - QueuePreprocessingFunc フィラー判定補助カラム追加

### 🎯 変更目的
フィラー候補（is_filler=1）の前後文構造を明示的に保存することで、スコア判定の検証や改善の手がかりとする。

### 📝 変更内容

#### 対象ファイル
- `SpeechToTextPipeline/function_app.py`

#### 対象関数
- `QueuePreprocessingFunc`

#### 追加ロジック
**ステップ2（フィラースコアリング）部分に以下を追加：**

**1. merged_text_with_prev の構築**
   - `line_no - 1`の `transcript_text_segment` の末尾から「。で区切られた最後の文」を取得
   - その末尾文と現在のフィラー文（line_no）のテキストを結合
   - 例：
     ```
     N-1: こんにちは。良い天気。
     N  : えっと。
     → merged_text_with_prev: 良い天気えっと。
     ```

**2. merged_text_with_next の構築**
   - `line_no + 1`の `transcript_text_segment` の先頭文を取得
   - 現在のフィラー文の末尾の「。」を除き、次の文と結合
   - 例：
     ```
     N  : えっと。
     N+1: 明日は雨です。
     → merged_text_with_next: えっと明日は雨です。
     ```

#### 実装コード
```python
# フィラー判定補助カラムの構築
merged_text_with_prev = ""
merged_text_with_next = ""

# merged_text_with_prev: 前のセグメントの最後の文 + 現在の文
if prev_text:
    prev_sentences = [s.strip() for s in prev_text.strip().split("。") if s.strip()]
    prev_last_sentence = prev_sentences[-1] if prev_sentences else ""
    merged_text_with_prev = prev_last_sentence + bracket_text

# merged_text_with_next: 現在の文（。を除く）+ 次のセグメントの最初の文
if next_text:
    next_sentences = [s.strip() for s in next_text.strip().split("。") if s.strip()]
    next_first_sentence = next_sentences[0] if next_sentences else ""
    merged_text_with_next = bracket_text.strip("。") + next_first_sentence

# DB更新（補助カラムも含めて）
cursor.execute("""
    UPDATE dbo.TranscriptProcessingSegments
    SET front_score = ?, after_score = ?, 
        merged_text_with_prev = ?, merged_text_with_next = ?,
        updated_datetime = GETDATE()
    WHERE meeting_id = ? AND line_no = ?
""", (front_score, back_score, merged_text_with_prev, merged_text_with_next, meeting_id, line_no))
```

### 🗂 影響テーブル
- `TranscriptProcessingSegments`
  - `merged_text_with_prev` (新規追加)
  - `merged_text_with_next` (新規追加)

### ✅ 期待効果
1. **検証性向上**: スコア計算に使用された前後文構造が明示的に保存される
2. **デバッグ支援**: フィラー判定の根拠を後から確認可能
3. **改善基盤**: スコアリングロジックの改善時に参考データとして活用

### 🔄 次のステップ
- テーブル定義の更新（`merged_text_with_prev`, `merged_text_with_next` カラム追加）
- 既存データの移行（必要に応じて）
- 補助カラムを活用した検証・分析機能の実装

## 2025年7月22日 - QueueMergingAndCleanupFunc 不具合修正

### 🎯 修正目的
ProcessedTranscriptSegments.merged_text に不自然な発話（例：「大丈。（夫です。）（大丈夫です。）」）が含まれる問題を解決。

### 🔍 問題の特定

#### 問題①: delete_candidate_word が適切に削除されない
- **現象**: `delete_candidate_word = "大丈。"` が補完元テキストから削除されず、補完後の文と共存
- **原因**: `replace()` 処理が句点やスペースのバリエーションに対応していない
- **影響**: 「大丈。」と「大丈夫です。」が両方残り、不自然な発話になる

#### 問題②: is_filler=True の行が統合対象に含まれる
- **現象**: フィラー行（例：「（夫です。）」）が `pre_merged_text_segments` に含まれてマージ対象になる
- **原因**: フィラー行の除外処理が不完全
- **影響**: 本来除外されるべきフィラー行が最終出力に含まれる

### 🛠️ 修正内容

#### 1. delete_candidate_word 削除処理の強化
```python
# 修正前
cleaned_prev_text = prev_transcript_text.replace(prev_delete_candidate_word, "")

# 修正後
import re
delete_pattern = re.escape(prev_delete_candidate_word.strip())
cleaned_prev_text = re.sub(f"{delete_pattern}[。]?\\s*", "", prev_transcript_text)
```

- **改善点**: 句点付き、スペース付き、句点のみなど様々なパターンに対応
- **対応パターン**: 「大丈。」「大丈 」「大丈」など

#### 2. フィラー行除外処理の強化
```python
# 補完処理後の重複チェックを追加
if is_filler:
    logging.info(f"[STEP4] Skipping filler line {line_no} after processing")
    continue
```

- **改善点**: フィラー行は補完処理後も確実に除外
- **効果**: フィラー行が `pre_merged_text_segments` に含まれることを防止

### ✅ 期待効果
1. **自然な発話生成**: delete_candidate_word が適切に削除され、補完後の文のみが残る
2. **フィラー行除外**: フィラー行が最終出力に含まれることがなくなる
3. **処理精度向上**: 句点やスペースのバリエーションに対応した柔軟な削除処理

### 🔄 次のステップ
- 修正後の出力品質の検証
- 他のフィラー補完パターンでの動作確認
- 必要に応じて正規表現パターンの調整

## 2025年7月16日 - TranscriptProcessingSegments フィラー補助カラム（merged_text_with_prev/next）空値問題の調査・修正

### 🎯 背景
TranscriptProcessingSegments の is_filler=1 の行で、merged_text_with_prev / merged_text_with_next がNULLとなるケースが発生。

### 🔍 原因調査
- 前後のセグメントが存在しない場合や、split("。")の結果が空リストとなる場合、結合値が空のままDBに保存されていた。
- 先頭・末尾行や「。」が含まれない発話など、文分割の境界ケースで空値となることが多かった。

### 🛠️ 修正内容
- 前後文が空や不正な場合も、必ず ""（空文字）を格納するよう明示的に修正。
- 各段階で詳細なデバッグログ（前後文・分割文・最終結合値）を出力し、原因特定・検証を容易にした。
- ロジック例：

```python
# merged_text_with_prev: 前のセグメントの最後の文 + 現在の文
if prev_text and prev_text.strip():
    prev_sentences = [s.strip() for s in prev_text.strip().split("。") if s.strip()]
    if prev_sentences:
        prev_last_sentence = prev_sentences[-1]
        merged_text_with_prev = prev_last_sentence + bracket_text
    else:
        merged_text_with_prev = ""
else:
    merged_text_with_prev = ""

# merged_text_with_next: 現在の文（。を除く）+ 次のセグメントの最初の文
if next_text and next_text.strip():
    next_sentences = [s.strip() for s in next_text.strip().split("。") if s.strip()]
    if next_sentences:
        next_first_sentence = next_sentences[0]
        merged_text_with_next = bracket_text.strip("。") + next_first_sentence
    else:
        merged_text_with_next = ""
else:
    merged_text_with_next = ""
```

- ログ例：
  - 前後文・分割文・結合結果・DB更新内容をINFO/WARNINGで出力

### ✅ 結果
- is_filler=True の全行で merged_text_with_prev / merged_text_with_next に必ず値（空文字含む）が格納されるようになった
- ログで原因特定・データ検証が容易になった

### 📝 参考
- 先頭・末尾行や「。」が含まれない発話も安全に処理される
- データ分析・補完精度改善の基盤として活用可能

## 2025年7月16日 - スコア判定対象を merged_text_with_prev / merged_text_with_next に統一

### 🎯 背景・目的
front_score / after_score のスコア判定に使用する文が曖昧で、merged_text_with_prev / merged_text_with_next との整合性が取れていなかった。これを解消し、「文脈結合後の文」を評価する明確な方針に統一。

### 🛠️ 変更内容
- スコア計算ロジックを下記のように修正：
  - front_score：merged_text_with_prev をスコア対象
  - back_score ：merged_text_with_next をスコア対象
  - どちらかが空の場合はスコア計算をスキップ
- ログも詳細に出力

```python
# merged_text_with_prev/nextを使用してスコア判定
front_score = 0.0
back_score = 0.0
if merged_text_with_prev and merged_text_with_prev.strip():
    try:
        front_score_result = evaluate_connection_naturalness_no_period("", "", merged_text_with_prev)
        front_score = front_score_result.get("front_score", 0.0)
    except Exception as e:
        front_score = 0.0
if merged_text_with_next and merged_text_with_next.strip():
    try:
        back_score_result = evaluate_connection_naturalness_no_period("", "", merged_text_with_next)
        back_score = back_score_result.get("back_score", 0.0)
    except Exception as e:
        back_score = 0.0
```

### ✅ 期待効果
- 文脈の整合性と補完結果の説明性が向上
- スコア計算の根拠が明確化し、分析・改善が容易に

## 2025年7月16日 - フィラー削除判定ロジックの整合性修正

### 🎯 背景・問題
TranscriptProcessingSegments の is_filler = True の行で、スコアと出力結果に矛盾が発生：
- front_score = 0.4（低い = 不自然）
- after_score = 0.6（高い = 自然）
- にもかかわらず revised_text_segment は「後接続パターン」に基づいた内容
- **「不自然な方が採用されてしまっている」**という逆転が起きていた

### 🔍 発見した問題点

**パターン①（スコア判定の逆転）:**
- 元のコード: `if front_score >= after_score:` 
- 問題: スコアが高い方（自然な方）を選択していた
- 修正: `if front_score <= after_score:` に変更し、スコアが低い方（不自然な方）を削除対象とする

**パターン②（merged_textの活用）:**
- 元のコード: 前後のセグメントを再取得して文を再構築
- 修正: 既に生成済みの `merged_text_with_prev/next` を直接使用

### 🛠️ 修正内容

**1. スコア判定の修正**: `front_score <= after_score` で判定（スコアが低い方を削除）
**2. merged_textの活用**: 既存の `merged_text_with_prev/next` を直接使用
**3. 詳細ログの追加**: 判定過程と結果を詳細に出力

```python
# スコアが低い方（不自然な方）を削除対象とする
if front_score <= after_score:
    # front_scoreが低い（不自然）→ merged_text_with_prevを使う
    revised_text = merged_text_with_prev
else:
    # after_scoreが低い（不自然）→ merged_text_with_nextを使う
    revised_text = merged_text_with_next
```

### ✅ 結果
- スコアと削除判定・補完生成結果の整合性が取れるようになった
- 不自然な文脈を削除し、自然な文脈を保持する正しい判定が可能に

## 2025年1月27日 - Queue Trigger ベースの非同期処理関数群実装

### 実装内容
- **PollingTranscriptionResults関数は編集せず**、新しいQueue Triggerベースの関数群を追加
- 4つのQueue Trigger関数を実装：
  - `QueuePreprocessingFunc` (queue-preprocessing)
  - `QueueMergingAndCleanupFunc` (queue-merging)  
  - `QueueSummarizationFunc` (queue-summary)
  - `QueueExportFunc` (queue-export)

### 各関数の責務
1. **QueuePreprocessingFunc**
   - ステップ1-3: セグメント化、フィラースコア、補完候補
   - 出力テーブル: `TranscriptProcessingSegments`
   - OpenAI API使用: ✅ (Step2)

2. **QueueMergingAndCleanupFunc**
   - ステップ4-6: セグメント統合、話者整形、OpenAIフィラー除去
   - 出力テーブル: `ProcessedTranscriptSegments`
   - OpenAI API使用: ✅ (Step6)

3. **QueueSummarizationFunc**
   - ステップ7: ブロック要約タイトル生成
   - 出力テーブル: `ConversationSummaries`
   - OpenAI API使用: ✅

4. **QueueExportFunc**
   - ステップ8: ConversationSummaries → ConversationSegments
   - 出力テーブル: `ConversationSegments`
   - OpenAI API使用: ❌

### フィラー判定補助カラム追加
- `merged_text_with_prev`: 前文との結合テキスト
- `merged_text_with_next`: 次文との結合テキスト
- これらのカラムを使ってOpenAI API（gpt-3.5-turbo）で自然さスコアを直接評価
- `front_score`と`after_score`を生成・保存

### フィラー削除判定ロジック修正
- **修正前**: スコアが高い方を削除対象にしていた（不整合）
- **修正後**: スコアが低い方（不自然な方）を削除対象とする
- `merged_text_with_prev`または`merged_text_with_next`のどちらかを`revised_text_segment`に保存

### OpenAI API直接利用
- `openai_completion_step2.py`の依存を断ち、関数内に直接記述
- `get_naturalness_score()`関数を実装
- 自然さスコア（0.0〜1.0）を算出

### 処理の非同期化
- 各ステップ完了時、次のQueueへ`send_queue_message()`を実行
- エラーハンドリングとステータス管理を実装
- ログの充実化

### 現在の判定対象
- ChatGPTに判定を依頼しているカラム: `merged_text_with_prev`と`merged_text_with_next`
- これらの文脈結合後の文を評価してスコアを算出し、判定に用いる

### 技術的改善点
- スコアと削除判定の整合性向上
- 検証性の向上（補助カラムによる前後文脈保持）
- 処理の非同期化によるスケーラビリティ向上
- エラーハンドリングの強化
- ログ出力の詳細化

### 今後の課題
- 各出力テーブルのCREATE TABLEマイグレーション整備
- PollingからのQueue送信ロジック整理
- 全ステップのstatus管理とロールバックポリシー設計

## 2025年1月27日 - スコア比較ロジックの修正（delete_candidate_word の生成条件を正しくする）

### 背景
- 現在のロジックでは、以下のような条件で delete_candidate_word の元文を決定している：
```python
if front_score <= after_score:
    delete_candidate_word = prev_last_sentence
else:
    delete_candidate_word = next_first_sentence
```
- これは スコアが高い方（より自然な文）を選ぶロジックとしては誤っている
- 本来は スコアが高い側の merged_text が採用され、その構成元文が delete_candidate_word に入るべき

### 修正内容

#### 1. QueuePreprocessingFunc 内のスコア比較ロジック修正
- **修正前**: `if front_score <= after_score:` → 前文の最後の文を削除候補
- **修正後**: `if front_score > after_score:` → 前文の最後の文を削除候補
- **修正前**: `else:` → 後文の最初の文を削除候補  
- **修正後**: `else:` → 後文の最初の文を削除候補

#### 2. QueueMergingAndCleanupFunc 内のスコア比較ロジック修正
- **修正前**: `if front_score <= after_score:` → merged_text_with_prev を選択
- **修正後**: `if front_score > after_score:` → merged_text_with_prev を選択

#### 3. 正しい条件分岐のロジック
```python
if front_score > after_score:
    # より自然なのは前文接続 → 前文の最後の文を delete_candidate_word に
    delete_candidate_word = prev_last_sentence
else:
    # より自然なのは後文接続 → 後文の最初の文を delete_candidate_word に
    delete_candidate_word = next_first_sentence
```

### 技術的改善点
- **ロジック整合性**: スコアが高い方（より自然な方）の merged_text が採用される
- **削除候補の正確性**: 採用された merged_text の構成元文が正しく delete_candidate_word に格納される
- **フィラー処理の精度向上**: より自然な文脈を保持し、不自然な部分のみを削除対象とする

### 期待される効果
- フィラー削除処理の精度向上
- 自然な文脈の保持
- 不適切な削除候補の選択を防止

## 2025年1月27日 - delete_candidate_word に格納する語句の語尾に「。」を追加

### 背景
- delete_candidate_word は、補完に使われた前後の文の一部を削除対象として記録するカラム
- 実際の文末には「。」がついているケースがほとんどであり、整合性・自然さの観点からも語尾の句点を明示的に追加すべき
- 補完対象に使われた語句が句点無しで記録されていると、後続ステップでの判定ミスや削除精度の低下につながる

### 修正内容

#### 1. QueuePreprocessingFunc 内の delete_candidate_word 生成処理修正
- **修正前**: `delete_candidate = prev_last_sentence` / `delete_candidate = next_first_sentence`
- **修正後**: `delete_candidate = prev_last_sentence.rstrip("。") + "。"` / `delete_candidate = next_first_sentence.rstrip("。") + "。"`

#### 2. 語尾句点の統一処理
```python
# 既に句点が含まれていた場合の二重付与を防ぐため、.rstrip("。") で一旦除去してから付け直す
delete_candidate = prev_last_sentence.rstrip("。") + "。"
delete_candidate = next_first_sentence.rstrip("。") + "。"
```

### 技術的改善点
- **整合性向上**: 常に文として閉じた形で記録される
- **削除精度向上**: 後続ステップでの判定ミスを防止
- **自然さ保持**: 実際の文構造に近い形で削除候補を記録

### 期待される効果
- フィラー削除処理の精度向上
- 後続ステップでの判定ミス防止
- 削除候補の整合性確保

## 2025年1月27日 - QueueMergingAndCleanupFunc の整合性確認と修正対応

### 背景
- QueuePreprocessingFunc の出力構造が変更され、以下のカラムが TranscriptProcessingSegments に追加された：
  - `merged_text_with_prev`
  - `merged_text_with_next`
  - `delete_candidate_word`
  - `front_score`
  - `after_score`
- 旧カラム `revised_text_segment` は廃止された
- QueueMergingAndCleanupFunc の整合性を確認し、必要に応じて修正が必要

### 修正内容

#### 1. revised_text_segment の参照確認
- **確認結果**: 現在のコードでは revised_text_segment を直接参照していないため、削除不要
- **対応**: 今後も使用禁止を継続

#### 2. merged_text_with_prev/next の活用強化
- **修正前**: 次の行のfiller情報のみを取得
- **修正後**: 現在の行の merged_text_with_prev/next を優先的に活用
- **ロジック**: 
  ```python
  if front_score > after_score and merged_text_with_prev and merged_text_with_prev.strip():
      current_merged_text = merged_text_with_prev
  elif merged_text_with_next and merged_text_with_next.strip():
      current_merged_text = merged_text_with_next
  ```

#### 3. delete_candidate_word の活用確認
- **確認結果**: 現在のコードで正しく活用されている
- **処理**: `transcript_text.replace(delete_word or "", "")` で不要な語句を削除

#### 4. 空文字対策の実装
- **追加**: `merged_text_with_prev/next` が空の場合の処理例外を実装
- **対策**: `.strip()` による空文字チェックと条件分岐の追加
- **ログ**: 空文字の場合の警告ログを追加

#### 5. ログの充実化
- **追加ログ**:
  - `[MERGING] Processing line {line_no}, speaker={speaker}`
  - `[MERGING] Using merged_text_with_prev/next`
  - `[MERGING] Using current_merged_text/next_merged_text/original text`
  - `[DB] Inserted ProcessedTranscriptSegment`
  - `[CLEANUP] Processing segment_id={segment_id}`
  - `[CLEANUP] Cleaned text`
  - `[DB] Updated ProcessedTranscriptSegment`

### 技術的改善点
- **データ活用の最適化**: 現在の行の merged_text_with_prev/next を優先的に使用
- **空文字対策**: 各段階で空文字チェックを実装
- **ログの詳細化**: 処理過程の可視性向上
- **エラーハンドリング**: 各段階での例外処理強化

### 期待される効果
- フィラー処理の精度向上
- データの整合性確保
- 処理過程の可視性向上
- エラー原因の特定容易化

## 2025年1月27日 - QueueMergingAndCleanupFunc 内でのフィラー削除処理のインライン化

### 背景
- 現在のQueueMergingAndCleanupFuncでは `from openai_processing.openai_completion_step6 import remove_fillers_from_text` でインポートしている
- 将来的に `openai_completion_step6.py` は廃止予定
- 依存関係を減らし、関数内に直接実装することで保守性を向上

### 修正内容

#### 1. インポート文の削除
- **修正前**: `from openai_processing.openai_completion_step6 import remove_fillers_from_text`
- **修正後**: インポート文を削除し、関数内に直接実装

#### 2. インライン関数の実装
- **関数名**: `remove_fillers_from_text_inline()`
- **機能**: もともとの `remove_fillers_from_text()` と完全に同じ
- **内容**:
  - システムメッセージ: フィラー削除の詳細な指示
  - 対象フィラー: 「えっと」「あの」「まあ」「その」「ですけど」など
  - 注意事項: 自然な会話の流れを崩さない
  - 出力: 修正後のテキストのみ

#### 3. OpenAI API呼び出しの実装
```python
response = client.chat.completions.create(
    model=os.environ.get("OPENAI_MODEL", "gpt-3.5-turbo"),
    messages=[
        {"role": "system", "content": system_message},
        {"role": "user", "content": user_message}
    ],
    temperature=0.1,  # 低い温度で一貫性を保つ
    max_tokens=200    # 短い応答に制限
)
```

#### 4. エラーハンドリングの維持
- **トークン使用量記録**: エラーハンドリング付きで実装
- **フォールバック機能**: 例外発生時は元のテキストを返す
- **ログ出力**: 処理過程の詳細な可視化

### 技術的改善点
- **依存関係の削減**: 外部ファイルへの依存を排除
- **保守性向上**: 関数内に全てのロジックが集約
- **将来性確保**: openai_completion_step6.py の廃止に備える
- **機能の完全保持**: もともとのロジックを100%踏襲

### 期待される効果
- 外部依存の削減
- コードの自己完結性向上
- 将来の廃止ファイルへの対応
- 保守性の向上

## 2025年1月27日 - 自然さ判定ステップのログ追加とスコア異常の原因調査・修正

### 背景
- 元のStep2（openai_completion_step2.py）では詳細なログ出力があったが、QueuePreprocessingFunc統合後はログが一切出力されていない
- `front_score`, `after_score`が常に`0.5`のままになるケースが発生
- OpenAI API応答が正しく取得・パースされていない可能性

### 修正内容

#### 1. `get_naturalness_score()`関数のログ強化
- **追加ログ**:
  - `[OpenAI] Empty text detected, returning default score 0.5`
  - `[OpenAI] Evaluating text: '{text}'`
  - `[OpenAI] Raw response: {response}`
  - `[OpenAI] Extracted content: '{content}'`
  - `[OpenAI] Parsed score: {score}`
  - `[OpenAI] API call failed: {e}`
  - `[OpenAI] Response structure: {response}`

#### 2. フィラー判定処理のログ強化
- **追加ログ**:
  - `[FILLER] meeting_id={meeting_id}, line_no={line_no}, text: '{text}'`
  - `[FILLER] Prev text (line {line_no - 1}): '{prev_text}'`
  - `[FILLER] Next text (line {line_no + 1}): '{next_text}'`
  - `[FILLER] Prev sentences: {prev_sentences}`
  - `[FILLER] merged_text_with_prev: '{merged_text_with_prev}'`
  - `[FILLER] Next sentences: {next_sentences}`
  - `[FILLER] merged_text_with_next: '{merged_text_with_next}'`
  - `[FILLER] Front score for merged_text_with_prev: {front_score}`
  - `[FILLER] Back score for merged_text_with_next: {back_score}`
  - `[FILLER] Final Scores - front: {front_score}, back: {back_score}`
  - `[FILLER] Updated line {line_no} with scores: front={front_score}, back={back_score}`

#### 3. 補完候補挿入処理のログ強化
- **追加ログ**:
  - `[REVISION] meeting_id={meeting_id}, line_no={line_no}, front_score={front_score}, after_score={after_score}`
  - `[REVISION] merged_text_with_prev: '{merged_text_with_prev}'`
  - `[REVISION] merged_text_with_next: '{merged_text_with_next}'`
  - `[REVISION] Using merged_text_with_prev (front_score={front_score} <= after_score={after_score})`
  - `[REVISION] Using merged_text_with_next (front_score={front_score} > after_score={after_score})`
  - `[REVISION] revised_text: '{revised_text}', delete_candidate: '{delete_candidate}'`

### 調査ポイント
1. **スコアが常に0.5になる原因調査**
   - `openai.ChatCompletion.create()`のレスポンスが正しく取得できているか
   - `response['choices'][0]['message']['content']`にスコアが正しく含まれているか
   - `float(content)`の変換に失敗して`except:`ブロックでfallbackの0.5が返っていないか

2. **ログ出力の復活・整備**
   - 各filler判定対象行について詳細なログを追加
   - meeting_id付加で個別テスト時の特定を容易化
   - 評価対象文・応答内容・スコアのトレーサビリティ確保

### 技術的改善点
- **デバッグ性向上**: ログレベルを統一し、問題箇所の特定を容易化
- **トレーサビリティ確保**: 評価対象文からスコア算出までの全過程をログ化
- **エラー原因特定**: OpenAI API応答の詳細ログで異常の早期発見
- **検証性向上**: 実際にどちらの文が評価対象になっているかが明確化

### 最終目的
- スコアが常に0.5になる異常を早期に特定・修正
- 評価対象文・応答内容・スコアのトレーサビリティを確保
- 自然さ評価の精度改善に活用

## 2025年1月27日 - OpenAI API呼び出し形式の新仕様対応

### 背景
- 現在の実装では`openai.ChatCompletion.create()`を使用しており、これは`openai>=1.0.0`では非対応
- そのため、API呼び出しが失敗し、スコアが常にfallbackの`0.5`になっている
- 一方、`openai_completion_step2.py`側の`client.chat.completions.create()`は正常に動作している

### 修正内容

#### 1. `get_naturalness_score()`関数の新仕様対応
- **修正前**: `openai.ChatCompletion.create()`を使用
- **修正後**: `openai.OpenAI()`からclientを初期化し、`client.chat.completions.create()`を使用

#### 2. API呼び出し形式の変更
```python
# 修正前
response = openai.ChatCompletion.create(
    model=os.getenv("OPENAI_MODEL", "gpt-3.5-turbo"),
    messages=[...],
    temperature=0
)
content = response['choices'][0]['message']['content'].strip()

# 修正後
client = openai.OpenAI(api_key=os.environ.get("OPENAI_API_KEY"))
response = client.chat.completions.create(
    model=os.environ.get("OPENAI_MODEL", "gpt-3.5-turbo"),
    messages=[...],
    temperature=0
)
content = response.choices[0].message.content.strip()
```

#### 3. プロンプトの改善
- **修正前**: 「不自然な語順、意味の不明瞭さ、接続のぎこちなさがある場合は低いスコアとしてください」
- **修正後**: 「語順、意味の流れ、文脈のつながりを考慮し」

#### 4. システムメッセージの改善
- **修正前**: 「あなたは日本語の文の自然さを採点するアシスタントです。」
- **修正後**: 「あなたは日本語の文の自然さを評価するAIです。」

### 技術的改善点
- **互換性向上**: `openai>=1.0.0`に対応したAPI呼び出し形式
- **エラー解決**: API呼び出し失敗によるスコア異常の修正
- **レスポンス処理**: 新しいレスポンス形式（`response.choices[0].message.content`）に対応
- **ログ維持**: 詳細なログ出力を維持し、デバッグ性を確保

### 期待される効果
- スコアが常に0.5になる問題の解決
- OpenAI APIの正常な呼び出しとレスポンス取得
- 自然さ評価の精度向上
- 他のOpenAI API呼び出し箇所との一貫性確保

## 2025年1月27日 - TranscriptProcessingSegmentsテーブルへの挿入確認ログ追加とスコア関連の冗長なログ削除

### 背景
- `QueuePreprocessingFunc`を実行しても`TranscriptProcessingSegments`テーブルにレコードが0件のままである事象が発生
- INSERTが実行されているか、meeting_idごとに確認できるようログを追加したい
- 一方で、スコア（front_score / after_score）の出力は正常であることが確認できたため、それに関するログは削除してOK

### 修正内容

#### 1. INSERT実行確認ログの追加
- **追加ログ**: `[DB] Inserted TranscriptProcessingSegment: meeting_id={meeting_id}, line_no={line_no}, speaker={speaker}`
- **位置**: `INSERT INTO TranscriptProcessingSegments`のすぐ下に追加
- **効果**: 処理対象の行が実際にDBに追加されたことがログで確認可能

#### 2. スコア関連の冗長なログ削除
- **削除対象**:
  - `[OpenAI] Empty text detected, returning default score 0.5`
  - `[OpenAI] Evaluating text: '{text}'`
  - `[OpenAI] Raw response: {response}`
  - `[OpenAI] Extracted content: '{content}'`
  - `[OpenAI] Parsed score: {score}`
  - `[OpenAI] Response structure: {response}`
  - `[FILLER] Front score for merged_text_with_prev: {front_score}`
  - `[FILLER] Back score for merged_text_with_next: {back_score}`
  - `[FILLER] Final Scores - front: {front_score}, back: {back_score}`
  - `[REVISION] merged_text_with_prev: '{merged_text_with_prev}'`
  - `[REVISION] merged_text_with_next: '{merged_text_with_next}'`
  - `[REVISION] revised_text: '{revised_text}', delete_candidate: '{delete_candidate}'`

#### 3. ログの簡素化
- **保持ログ**:
  - `[FILLER] Processing line {line_no}, text: '{text}'`
  - `[FILLER] Updated line {line_no} with scores: front={front_score}, back={back_score}`
  - `[REVISION] Processing line {line_no}, front_score={front_score}, after_score={after_score}`
  - エラー時のログ（`[OpenAI] API call failed: {e}`など）

### 技術的改善点
- **デバッグ効率向上**: INSERT処理の可視化により、データが登録されない問題を正確にトレース可能
- **ログ可読性向上**: 不要なログを削除して、重要な情報に集中
- **パフォーマンス向上**: 冗長なログ出力を削減
- **問題特定の容易化**: meeting_id付加で個別テスト時の特定を容易化

### 目的
- INSERT処理が実行されているかの可視化により、データが登録されない問題を正確にトレース可能にする
- 不要なログを削除して、ログ全体の可読性とデバッグ効率を高める

## 2025年1月27日 - revised_text_segmentカラムの使用停止とdelete_candidate_wordの目的変更

### 背景
- `revised_text_segment`は、`merged_text_with_prev`または`merged_text_with_next`のどちらか一方と常に同じ内容であり、冗長
- また、補完後の選択文は明示的にスコアによって決まるため、必要であればその場で動的に取得可能
- カラムを残しておくことで、誤って処理対象として使われるリスクがある
- フィラー処理後に、元の`transcript_text_segment`に含まれる補助的な語句（例：「大丈。」などの言いよどみ）が文中に残ってしまうことがある

### 修正内容

#### 1. `revised_text_segment`カラムの使用停止
- **修正前**: `revised_text_segment`に選択されたテキストを保存
- **修正後**: `revised_text_segment`へのUPDATE処理をすべて削除
- **理由**: `merged_text_with_prev`/`merged_text_with_next`で代替可能

#### 2. `delete_candidate_word`の目的変更
- **修正前**: 前後のセグメントから削除する文を記録
- **修正後**: 補完により不要となる残留語句を記録
- **処理ロジック**:
  ```python
  # 元のテキストから選択されたテキストを除いた残りを削除候補とする
  if selected_text in original_text:
      delete_candidate_word = original_text.replace(selected_text, "").strip()
  else:
      delete_candidate_word = bracket_text  # 選択テキストが見つからない場合は元の括弧内テキスト
  ```

#### 3. QueueMergingAndCleanupFuncの修正
- **修正前**: `revised_text_segment`を参照して補完テキストを取得
- **修正後**: `merged_text_with_prev`/`merged_text_with_next`とスコアを参照して動的に選択
- **処理ロジック**:
  ```python
  # スコアに基づいて選択されたテキストを使用
  if front_score <= after_score:
      next_merged_text = next_segment[3]  # merged_text_with_prev
  else:
      next_merged_text = next_segment[4]  # merged_text_with_next
  ```

### 技術的改善点
- **テーブル構造の簡素化**: 冗長なカラムを削除し、データの整合性を向上
- **処理ロジックの明確化**: スコアに基づく選択ロジックを動的に実行
- **残留語句の除去**: 補完後に残る不要な断片語を自動削除
- **保守性の向上**: 処理とテーブル構造の簡素化により、補完ロジックのトレーサビリティと保守性を向上

### 目的
- 処理とテーブル構造の簡素化により、補完ロジックのトレーサビリティと保守性を向上させる
- 補完後の整形結果に不要な断片語が残ることを防止し、最終的なMergingAndCleanupStepによる削除処理と連携するための中間情報を記録する

## 2025年1月27日 - delete_candidate_wordの生成ロジック修正

### 背景
- `delete_candidate_word`はこれまで誤ってフィラー自身（transcript_text_segment）を格納していた
- 本来は「補完に使われた周辺文（merged_text_with_prev / next の構成元）」を明示的に記録する目的で使用する
- 補完に使われた文の構成元を正確に特定し、後続のクリーンアップ処理で削除対象を明確にする必要がある

### 修正内容

#### 1. `delete_candidate_word`の生成ロジック変更
- **修正前**: 元のテキストから選択されたテキストを除いた残りを削除候補とする
- **修正後**: 補完に使われた文の構成元を直接格納する

#### 2. 処理ルールの明確化
| 採用された補完方向 | delete_candidate_word に格納する内容 |
|-------------------|--------------------------------------|
| `merged_text_with_prev` 採用時 | conversation_segments[i-1]['text'] の **最後の文**（merged_text_with_prev 作成時に使用） |
| `merged_text_with_next` 採用時 | conversation_segments[i+1]['text'] の **最初の文**（merged_text_with_next 作成時に使用） |

#### 3. 実装ロジック
```python
# 前後の文から構成元を抽出
prev_last_sentence = ""
next_first_sentence = ""

if prev_text and prev_text.strip():
    prev_sentences = [s.strip() for s in prev_text.strip().split("。") if s.strip()]
    if prev_sentences:
        prev_last_sentence = prev_sentences[-1]

if next_text and next_text.strip():
    next_sentences = [s.strip() for s in next_text.strip().split("。") if s.strip()]
    if next_sentences:
        next_first_sentence = next_sentences[0]

# スコアに基づいて補完に使われた文を特定し、その構成元をdelete_candidate_wordに格納
if front_score <= after_score:
    # front_scoreが低い（不自然）→ merged_text_with_prevが採用された
    delete_candidate = prev_last_sentence  # 前の文の最後の文を削除候補とする
else:
    # after_scoreが低い（不自然）→ merged_text_with_nextが採用された
    delete_candidate = next_first_sentence  # 次の文の最初の文を削除候補とする
```

### 技術的改善点
- **正確性の向上**: 補完に使われた文の構成元を正確に特定
- **トレーサビリティの確保**: どの文が補完に使われ、どの文が削除対象かを明確化
- **処理の一貫性**: merged_text_with_prev/nextの生成ロジックと連動
- **後続処理との連携**: クリーンアップ処理で削除対象を正確に特定可能

### 目的
- 補完に使われた周辺文の構成元を明示的に記録し、後続のクリーンアップ処理と連携する
- フィラー処理のトレーサビリティを向上させ、削除対象の特定を正確に行う

## 2025年1月27日 - QueueSummarizationFunc ログ強化とメッセージ送信改善

### 🎯 変更目的
QueueSummarizationFunc のPoison Queue問題を解決するため、詳細なログ追加とメッセージ送信の信頼性向上を実施。

### 📝 変更内容

#### 対象ファイル
- `SpeechToTextPipeline/function_app.py`
- `SpeechToTextPipeline/openai_processing/openai_completion_step7.py`

#### 1. QueueSummarizationFunc ログ強化

**追加されたログポイント：**

**1. 関数開始時の meeting_id ログ出力**
```python
logging.info(f"=== QueueSummarizationFunc 開始: meeting_id={meeting_id} ===")
```

**2. ProcessedTranscriptSegments 抽出直後**
```python
logging.info(f"[DEBUG] ProcessedTranscriptSegments 行数: {len(rows)}")
if not rows:
    logging.warning(f"[WARN] meeting_id={meeting_id} のセグメントが存在しません。")
```

**3. ブロックごとの OpenAI API 呼び出し直前**
```python
logging.info(f"[DEBUG] OpenAIへ要約依頼: block_index={i}, 発話数={len(lines_only)}")
logging.info(f"[DEBUG] conversation_text サンプル: {conversation_text[:100]}...")
```

**4. OpenAI 応答後のログ**
```python
logging.info(f"[DEBUG] OpenAI 応答 title: {title}")
```

**5. ConversationSummaries への summary 挿入後**
```python
logging.info(f"[DB] Summary挿入完了: meeting_id={meeting_id}, offset={block['start_offset']}")
```

**6. 各セグメント挿入時**
```python
logging.info(f"[INSERT] Seg: speaker={speaker}, offset={offset}, content='{content[:30]}...'")
```

**7. 最終メッセージ送信前確認（queue-exportへ）**
```python
account_name_match = re.search(r'AccountName=([^;]+)', conn_str)
if account_name_match:
    logging.info(f"[DEBUG] queue-export送信先: {account_name_match.group(1)}")
logging.info(f"[DEBUG] queue-export メッセージ送信準備完了")
```

**8. 例外発生時の完全なスタックトレース**
```python
logging.error(f"[EXCEPTION] queue_summarization_func failed for meeting_id={meeting_id}")
logging.exception(f"❌ QueueSummarizationFunc エラー (meeting_id={meeting_id if 'meeting_id' in locals() else 'unknown'}): {e}")
```

#### 2. extract_offset_from_line 関数のロバスト化

**修正前：**
```python
match = re.match(r"(Speaker\d+: .+?)\(([\d.]+)\)$", line)
if match:
    body = match.group(1).rstrip()
    offset = float(match.group(2))
    return body, offset
else:
    return line, None
```

**修正後：**
```python
# 最後の括弧内が整数または小数の形式 "(12)" "(12.5)" に一致
match = re.search(r"\((\d+(?:\.\d+)?)\)\s*$", line)
if not match:
    return line, None  # offsetなし行

offset = float(match.group(1))
# 末尾の (数値) を除去して本文を取得
body = re.sub(r"\(\d+(?:\.\d+)?\)\s*$", "", line).strip()
return body, offset
```

#### 3. cleaned_text 復元ロジックの修正

**修正前：**
```python
speaker = int(line.split(":")[0].replace("Speaker", ""))
content = line.split(":")[1].split("(")[0].strip()
```

**修正後：**
```python
# segment_id から元の cleaned_text を再取得
cursor.execute("""
    SELECT speaker, cleaned_text, offset_seconds 
    FROM dbo.ProcessedTranscriptSegments 
    WHERE id = ?
""", (seg_id,))
seg_row = cursor.fetchone()

if not seg_row:
    logging.warning(f"[WARN] ProcessedTranscriptSegments にid={seg_id}が見つかりません")
    continue

speaker, cleaned_text, offset = seg_row
content = cleaned_text
```

#### 4. send_queue_message 関数のBase64エンコード対応

**修正前：**
```python
# メッセージをJSON文字列に変換
message_json = json.dumps(message)
queue_client.send_message(message_json)
```

**修正後：**
```python
# メッセージをJSON文字列に変換し、明示的にBase64エンコード
json_message = json.dumps(payload)
base64_encoded = base64.b64encode(json_message.encode("utf-8")).decode("utf-8")
queue_client.send_message(base64_encoded)
```

#### 5. QueueExportFunc 受信ログ追加

**追加されたログ：**
```python
# 受信メッセージのログ追加
raw_message = message.get_body().decode('utf-8')
logging.info(f"[DEBUG] Raw message: {raw_message}")
```

### 🗂 影響範囲

#### 対象関数
- `QueueSummarizationFunc`
- `QueueExportFunc`
- `send_queue_message`
- `extract_offset_from_line`

#### 対象テーブル
- `ProcessedTranscriptSegments`
- `ConversationSummaries`

### ✅ 期待効果

#### 1. Poison Queue問題の解決
- 詳細なログによりエラー発生箇所を特定
- Base64エンコードによりメッセージ送信の信頼性向上
- `extract_offset_from_line`のロバスト化により処理漏れを削減

#### 2. データ品質の向上
- `cleaned_text`の正確な復元
- 複雑な括弧パターンでの正確なoffset抽出
- エラーハンドリングの強化

#### 3. デバッグ能力の向上
- 処理ステップごとの詳細ログ
- メッセージ送受信の可視化
- エラー発生時の完全なスタックトレース

### 🔄 次のステップ
- ログ出力の監視とPoison Queue問題の解決確認
- 必要に応じた追加のログ強化
- パフォーマンス監視と最適化

## 2025年1月27日 - Polling停止とイベント駆動処理フローの実装

### 🎯 変更目的
不安定なポーリング処理を廃止し、Speech-to-Textジョブ完了と同時に次のステップを開始するイベント駆動の安定した処理フローに変更。

### 📝 変更内容

#### 対象ファイル
- `SpeechToTextPipeline/function_app.py`

#### 1. PollingTranscriptionResults の停止

**修正前：**
```python
@app.function_name(name="PollingTranscriptionResults")
@app.schedule(schedule="0 */5 * * * *", arg_name="timer", run_on_startup=False, use_monitor=False)
def polling_transcription_results(timer: func.TimerRequest) -> None:
```

**修正後：**
```python
# PollingTranscriptionResults を停止（イベント駆動に変更）
# @app.function_name(name="PollingTranscriptionResults")
# @app.schedule(schedule="0 */5 * * * *", arg_name="timer", run_on_startup=False, use_monitor=False)
# def polling_transcription_results(timer: func.TimerRequest) -> None:
```

#### 2. TriggerTranscriptionJob へのメッセージ送信追加

**追加された処理：**
```python
# Speech-to-Text ジョブ登録完了後、queue-preprocessing へメッセージ送信
try:
    send_queue_message("queue-preprocessing", {"meeting_id": meeting_id})
    logging.info(f"✅ queue-preprocessing へメッセージ送信完了: meeting_id={meeting_id}")
except Exception as queue_error:
    logging.error(f"❌ queue-preprocessing へのメッセージ送信失敗: {queue_error}")
    # メッセージ送信失敗でも処理は継続（後で手動で再実行可能）
```

#### 3. send_queue_message 関数の安全仕様実装

**修正前：**
```python
def send_queue_message(queue_name: str, payload: dict):
    try:
        import base64
        
        queue_service = get_queue_service_client()
        queue_client = queue_service.get_queue_client(queue_name)
        
        # メッセージをJSON文字列に変換し、明示的にBase64エンコード
        json_message = json.dumps(payload)
        base64_encoded = base64.b64encode(json_message.encode("utf-8")).decode("utf-8")
        queue_client.send_message(base64_encoded)
        
        logging.info(f"✅ キュー '{queue_name}' に送信成功: {json_message}")
    except Exception as e:
        logging.exception(f"[ERROR] キュー '{queue_name}' へのメッセージ送信に失敗しました")
        raise
```

**修正後：**
```python
def send_queue_message(queue_name: str, payload: dict):
    try:
        import base64
        from azure.storage.queue import QueueClient
        
        # 接続文字列を取得
        conn_str = os.environ.get("AzureWebJobsStorage")
        if not conn_str:
            raise ValueError("AzureWebJobsStorage 環境変数が設定されていません")
        
        # キュークライアントを作成
        queue_client = QueueClient.from_connection_string(conn_str, queue_name)
        
        # メッセージをJSON文字列に変換し、明示的にBase64エンコード
        json_message = json.dumps(payload)
        base64_encoded = base64.b64encode(json_message.encode("utf-8")).decode("utf-8")
        
        # メッセージを送信
        queue_client.send_message(base64_encoded)
        
        logging.info(f"✅ キュー '{queue_name}' に送信成功: {json_message}")
    except Exception as e:
        logging.exception(f"[ERROR] キュー '{queue_name}' へのメッセージ送信に失敗しました")
        raise
```

### 🗂 影響範囲

#### 対象関数
- `PollingTranscriptionResults`（停止）
- `TriggerTranscriptionJob`（メッセージ送信追加）
- `send_queue_message`（安全仕様実装）

#### 対象キュー
- `queue-preprocessing`

### ✅ 期待効果

#### 1. イベント駆動の安定した処理フロー
- 不安定なポーリング処理を廃止
- Speech-to-Textジョブ完了と同時に次のステップを開始
- 全ステップがキューを通じてつながる

#### 2. メッセージ送信の信頼性向上
- 明示的なBase64エンコードによりAzure Queue Storageでの確実な保存
- 接続文字列の検証
- エラーハンドリングの強化

#### 3. トランザクション追跡の改善
- 各ステップの開始・完了が明確
- 障害切り分けが容易
- 手動での再実行が可能

### 🔄 新しい処理フロー

#### イベント駆動フロー
1. **Blob Upload** → EventGrid Trigger
2. **TriggerTranscriptionJob** → Speech-to-Text登録
3. **Speech-to-Text完了** → Callback URL
4. **TriggerTranscriptionJob** → `queue-preprocessing`送信
5. **QueuePreprocessingFunc** → `queue-merging`送信
6. **QueueMergingAndCleanupFunc** → `queue-summary`送信
7. **QueueSummarizationFunc** → `queue-export`送信
8. **QueueExportFunc** → 完了

#### ログ出力例
```
=== Transcription Job Trigger Start ===
Blob URL: https://storage.blob.core.windows.net/container/meeting_102_user_1_audio.wav
🎯 Extracted meeting_id=102, user_id=1
✅ SAS URL 生成成功: https://...
📏 file_size=1234567 bytes, duration_seconds=300 sec
🆔 Transcription Job ID: abc123
✅ Meetings テーブルにレコード挿入完了
✅ queue-preprocessing へメッセージ送信完了: meeting_id=102
```

### 🔄 次のステップ
- イベント駆動フローの動作確認
- メッセージ送信の信頼性監視
- 必要に応じた追加のエラーハンドリング強化

## 2025年1月27日 - SASトークン取得API統一化

### 🎯 変更目的
音声アップロード機能全般でSASトークン取得APIの呼び出し方式をPOST + JSONボディ形式に統一し、API使用方法の一貫性と安定性を確保。

### 📝 変更内容

#### 対象ファイル
- `next-app/src/hooks/useRecording.ts`
- `next-app/src/app/api/azure/get-sas-token/route.ts`

#### 1. useRecording.tsの修正

**修正前**:
```typescript
const sasResponse = await fetch(`/api/azure/get-sas-token?fileName=${encodeURIComponent(fileName)}`)
```

**修正後**:
```typescript
const sasResponse = await fetch('/api/azure/get-sas-token', {
  method: 'POST',
  headers: {
    'Content-Type': 'application/json',
  },
  body: JSON.stringify({ fileName })
})
```

#### 2. get-sas-token APIの修正

**追加内容**:
- POSTメソッドハンドラーを追加
- リクエストボディから`fileName`を取得する処理を実装
- CORSヘッダーにPOSTメソッドを追加

**追加されたPOSTハンドラー**:
```typescript
export async function POST(request: NextRequest) {
  try {
    console.log('SASトークン生成開始 (POST)')
    
    // リクエストボディからファイル名を取得
    const body = await request.json()
    const fileName = body.fileName
    
    if (!fileName) {
      console.error('ファイル名が指定されていません')
      return NextResponse.json(
        { error: 'ファイル名が指定されていません' },
        { status: 400 }
      )
    }
    
    // ... 既存のSASトークン生成ロジック
  } catch (error) {
    // ... エラーハンドリング
  }
}
```

### 🗂 影響範囲
- 音声アップロード機能全般
- 録音画面と商談作成画面の両方で同じAPI呼び出し方式を使用

### ✅ 期待効果
1. **一貫性の確保**: 音声アップロードの全ページで同じAPI呼び出し方式を使用
2. **バグの防止**: GET/POST混在による将来的なバグを防止
3. **メンテナンス性向上**: 統一されたAPI使用方法により保守性が向上
4. **エラー解消**: "ファイル名が指定されていません"エラーの解消

### 🔄 次のステップ
- 統一化後のAPI動作確認
- 全ページでの音声アップロード機能テスト
- 必要に応じた追加のエラーハンドリング強化

## 2025年1月27日 - TriggerTranscriptionJob 文字起こしロジック統合とQueueメッセージ送信改善

### 🎯 変更目的
`PollingTranscriptionResults`の廃止に伴い、非同期STTジョブの完了確認・文字起こし結果の保存処理が消失していた問題を解決。`TriggerTranscriptionJob`に文字起こしロジックを統合し、Queueメッセージ送信の改善を実施。

### 📝 変更内容

#### 対象ファイル
- `SpeechToTextPipeline/function_app.py`

#### 1. TriggerTranscriptionJob 文字起こしロジック統合

**追加された機能**:
- **ポーリングループ**: 最大20回、15秒間隔でジョブ完了を確認
- **文字起こし結果取得**: `contenturl_0.json`から`recognizedPhrases`を取得
- **transcript_text合成**: 話者、テキスト、オフセットを含む形式で合成
- **DB更新**: `Meetings`テーブルの`transcript_text`と`status`を更新

**実装コード**:
```python
# ポーリングループ
max_retries = 20
sleep_seconds = 15
transcription_url = f"https://{region}.api.cognitive.microsoft.com/speechtotext/v3.0/transcriptions/{job_id}"

for i in range(max_retries):
    status_resp = requests.get(transcription_url, headers=headers)
    job_status = status_resp.json().get("status")
    
    if job_status == "Succeeded":
        break
    elif job_status in ["Failed", "Canceled"]:
        # エラー処理
        return func.HttpResponse(f"Transcription failed: {job_status}", status_code=500)
    
    time.sleep(sleep_seconds)
else:
    # タイムアウト処理
    return func.HttpResponse("Timeout waiting for transcription result", status_code=504)

# 文字起こし結果取得とDB更新
if job_status == "Succeeded":
    # transcriptionファイル取得
    files_url = f"{transcription_url}/files"
    files_resp = requests.get(files_url, headers=headers)
    files_data = files_resp.json()

    transcription_files = [
        f for f in files_data["values"]
        if f.get("kind") == "Transcription" and f.get("name", "").startswith("contenturl_0")
    ]
    
    # transcript_text合成
    transcript = []
    for phrase in result_json["recognizedPhrases"]:
        speaker = phrase.get("speaker", "Unknown")
        text = phrase["nBest"][0]["display"]
        offset = phrase.get("offset", "PT0S")
        try:
            offset_seconds = round(isodate.parse_duration(offset).total_seconds(), 1)
        except:
            offset_seconds = 0.0
        transcript.append(f"(Speaker{speaker})[{text}]({offset_seconds})")
    transcript_text = " ".join(transcript)

    # DB更新
    cursor.execute("""
        UPDATE dbo.Meetings
        SET transcript_text = ?, status = 'transcribed',
            updated_datetime = GETDATE(), end_datetime = GETDATE()
        WHERE meeting_id = ? AND user_id = ?
    """, (transcript_text, meeting_id, user_id))
    conn.commit()
```

#### 2. send_queue_message 関数のログ強化

**追加されたログ**:
```python
# デバッグ用ログ：送信前のメッセージ情報
logging.info(f"📤 キュー送信準備: queue_name='{queue_name}', payload_type={type(payload)}, payload={payload}")

# デバッグ用ログ：変換後の情報
logging.info(f"📤 メッセージ変換: json_message='{json_message}', base64_length={len(base64_encoded)}")
```

#### 3. TriggerTranscriptionJob メッセージ送信改善

**修正前**:
```python
send_queue_message("queue-preprocessing", {"meeting_id": meeting_id})
```

**修正後**:
```python
message = {"meeting_id": meeting_id, "user_id": user_id}
send_queue_message("queue-preprocessing", message)
```

### 🗂 影響範囲

#### 対象関数
- `TriggerTranscriptionJob`
- `send_queue_message`

#### 対象テーブル
- `Meetings`（transcript_text、status更新）

### ✅ 期待効果

#### 1. 文字起こし処理の統合
- 非同期STTジョブの完了確認を`TriggerTranscriptionJob`単独で完結
- `transcript_text`を生成・保存することで、後続処理に必要なデータを供給
- エラーハンドリングの改善（失敗・タイムアウト時の適切な状態管理）

#### 2. Queueメッセージ送信の改善
- メッセージ送信過程の詳細な可視化
- `user_id`を含む完全なメッセージ送信
- デバッグ情報の充実による問題特定の容易化

#### 3. 処理の安全性向上
- 適切なHTTPレスポンス形式の使用
- ポーリングの確実な停止
- ステータスコードの明確化（200/500/504）

### 🔄 次のステップ
- 統合後の文字起こし処理の動作確認
- Queueメッセージ送信の信頼性監視
- 必要に応じた追加のエラーハンドリング強化 